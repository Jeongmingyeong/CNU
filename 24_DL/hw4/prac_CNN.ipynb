{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d2791e",
   "metadata": {},
   "source": [
    "## Prac1. Basic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5f9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61e8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## GPU/CPU Setting\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107f0cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## import dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform = transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform = transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afcbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size= batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7654f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100, 3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print dataset form and image\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    x, y = data\n",
    "    print(y.shape)\n",
    "    print(x.shape) # batch_size, channel, width, height\n",
    "    print(x[0].shape)\n",
    "    print(y[0])\n",
    "    plt.imshow(x[0].permute(1, 2, 0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2f9eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 64, 34, 34])\n",
      "torch.Size([1, 64, 17, 17])\n",
      "torch.Size([1, 128, 7, 7])\n",
      "torch.Size([1, 128, 3, 3])\n",
      "torch.Size([1, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convolutional layers\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=1)\n",
    "conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=0)\n",
    "print(conv1)\n",
    "print(conv2)\n",
    "\n",
    "input_data = torch.Tensor(1, 3, 32, 32)\n",
    "print(input_data.shape)\n",
    "\n",
    "print(conv1(input_data).shape)\n",
    "\n",
    "pool = nn.MaxPool2d(2)\n",
    "print(pool(conv1(input_data)).shape)\n",
    "\n",
    "final_output = conv2(pool(conv1(input_data)))\n",
    "print(final_output.shape)\n",
    "\n",
    "final_output = pool(final_output)\n",
    "print(final_output.shape)\n",
    "\n",
    "out = final_output.view(final_output.shape[0], -1)\n",
    "print(out.shape)\n",
    "\n",
    "fc = nn.Linear(1152, 10)\n",
    "fc(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87108287",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=0),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc = nn.Linear(3*3*128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bd1faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=1152, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5f747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372dfc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.56840\n",
      "Epoch 2 Loss 1.17599\n",
      "Epoch 3 Loss 1.07852\n",
      "Epoch 4 Loss 1.01596\n",
      "Epoch 5 Loss 0.96814\n",
      "Epoch 6 Loss 0.93106\n",
      "Epoch 7 Loss 0.89896\n",
      "Epoch 8 Loss 0.86813\n",
      "Epoch 9 Loss 0.84103\n",
      "Epoch 10 Loss 0.81512\n"
     ]
    }
   ],
   "source": [
    "## Model Train\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    average_cost = 0\n",
    "    \n",
    "    for batch_idx, (x_data, y_label) in enumerate(train_loader):\n",
    "        num_of_mini_batch = len(train_loader)\n",
    "        x_data = x_data.to(device)\n",
    "        input_image = x_data.reshape(batch_size, 3, 32, 32)\n",
    "        label = y_label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_predict = model(input_image)\n",
    "        loss = criterion(y_predict, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        average_cost = average_cost + (loss.item() / num_of_mini_batch)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "    print(\"Epoch {} Loss {:.5f}\".format((epoch_num+1), average_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3827e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy 9.36000%\n"
     ]
    }
   ],
   "source": [
    "## validation\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_total_data = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.reshape(batch_size, 3, 32, 32)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        predicted = torch.argmax(outputs_softmax, dim=1)\n",
    "        \n",
    "        num_total_data += len(images)\n",
    "        \n",
    "        answer = sum(label == predicted).item()\n",
    "        correct += answer\n",
    "        \n",
    "print(\"Model accuracy {:.5f}%\".format((correct / num_total_data) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1b81c",
   "metadata": {},
   "source": [
    "## Prac2. ResNet Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140be64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmingyeong/anaconda3/envs/ML/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/jeongmingyeong/anaconda3/envs/ML/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model 불러오기 (학습된 weight까지 불러오겠다.)\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c132cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model을 우리 task에 맞게 재정의\n",
    "# model.fc의 in_feature는 그대로 사용하고\n",
    "# out_feature를 1000개가 아닌 10개로 출력해주는 모델로 정의\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77eb3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7921e59c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m model(input_image)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_predict, label)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m average_cost \u001b[38;5;241m=\u001b[39m average_cost \u001b[38;5;241m+\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m num_of_mini_batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Model Train\n",
    "## 변경한 구조의 weight 재학습\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    average_cost = 0\n",
    "    \n",
    "    for batch_idx, (x_data, y_label) in enumerate(train_loader):\n",
    "        num_of_mini_batch = len(train_loader)\n",
    "        x_data = x_data.to(device)\n",
    "        input_image = x_data.reshape(batch_size, 3, 32, 32)\n",
    "        label = y_label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_predict = model(input_image)\n",
    "        loss = criterion(y_predict, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        average_cost = average_cost + (loss.item() / num_of_mini_batch)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "    print(\"Epoch {} Loss {:.5f}\".format((epoch_num+1), average_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_total_data = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.reshape(batch_size, 3, 32, 32)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        predicted = torch.argmax(outputs_softmax, dim=1)\n",
    "        \n",
    "        num_total_data += len(images)\n",
    "        \n",
    "        answer = sum(label == predicted).item()\n",
    "        correct += answer\n",
    "        \n",
    "print(\"Model accuracy {:.5f}\".format((correct / num_total_data) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6d6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
